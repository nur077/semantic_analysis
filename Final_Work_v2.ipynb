{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport nltk\n#import wandb\nimport torch\nimport random\nimport string\nimport requests\nimport numpy as np\nimport pandas as pd\nimport contractions\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom tensorflow import keras\nfrom textblob import TextBlob\nfrom tqdm.notebook import tqdm\nfrom keras import regularizers\nfrom string import punctuation\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom keras.models import Sequential\nfrom tqdm.keras import TqdmCallback\nfrom transformers import BertTokenizer\nfrom keras.utils import to_categorical\nfrom transformers import EvalPrediction\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.models import Model\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import TensorDataset\nfrom transformers import TFDistilBertModel\nfrom keras.layers import BatchNormalization\nfrom transformers import DistilBertTokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\nfrom keras.preprocessing.text import Tokenizer\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import BatchNormalization\nfrom transformers import BertForSequenceClassification\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import GRU, Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, TFDistilBertForSequenceClassification\nfrom keras.layers import Input, Dense, Embedding, Dropout, Reshape, Flatten, LSTM, Bidirectional\nfrom transformers import AutoModel, BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification\n\n\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:57:55.826315Z","iopub.execute_input":"2023-05-23T11:57:55.827432Z","iopub.status.idle":"2023-05-23T11:58:21.662968Z","shell.execute_reply.started":"2023-05-23T11:57:55.827381Z","shell.execute_reply":"2023-05-23T11:58:21.662052Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"pip install contractions","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:58:49.042347Z","iopub.execute_input":"2023-05-23T11:58:49.046345Z","iopub.status.idle":"2023-05-23T11:59:03.593313Z","shell.execute_reply.started":"2023-05-23T11:58:49.046290Z","shell.execute_reply":"2023-05-23T11:59:03.592052Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nCollecting textsearch>=0.0.21\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nCollecting anyascii\n  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pyahocorasick\n  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/new-train/train.csv')\ntest_df = pd.read_csv('/kaggle/input/new-test/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:03.602607Z","iopub.execute_input":"2023-05-23T11:59:03.602943Z","iopub.status.idle":"2023-05-23T11:59:03.761567Z","shell.execute_reply.started":"2023-05-23T11:59:03.602909Z","shell.execute_reply":"2023-05-23T11:59:03.760453Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[['text', 'sentiment']]\ntest_df = test_df[['text', 'sentiment']]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:03.765539Z","iopub.execute_input":"2023-05-23T11:59:03.765872Z","iopub.status.idle":"2023-05-23T11:59:03.786186Z","shell.execute_reply.started":"2023-05-23T11:59:03.765844Z","shell.execute_reply":"2023-05-23T11:59:03.784984Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"replace from positive, neutral, positive to 2, 1, 0 in column sentiment","metadata":{}},{"cell_type":"code","source":"train_df['sentiment'] = train_df['sentiment'].replace(['positive','neutral','negative'], [2, 1, 0])\ntest_df['sentiment'] = test_df['sentiment'].replace(['positive','neutral','negative'], [2, 1, 0])","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:03.788538Z","iopub.execute_input":"2023-05-23T11:59:03.789054Z","iopub.status.idle":"2023-05-23T11:59:03.823516Z","shell.execute_reply.started":"2023-05-23T11:59:03.789015Z","shell.execute_reply":"2023-05-23T11:59:03.822348Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = train_df['text'], train_df['sentiment']\nX_test, y_test = test_df['text'], test_df['sentiment']","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:03.825072Z","iopub.execute_input":"2023-05-23T11:59:03.825454Z","iopub.status.idle":"2023-05-23T11:59:03.831939Z","shell.execute_reply.started":"2023-05-23T11:59:03.825405Z","shell.execute_reply":"2023-05-23T11:59:03.831055Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"ratio of the sentiment","metadata":{}},{"cell_type":"code","source":"plt.hist(test_df['sentiment'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:46:33.310952Z","iopub.execute_input":"2023-05-23T14:46:33.311958Z","iopub.status.idle":"2023-05-23T14:46:33.646068Z","shell.execute_reply.started":"2023-05-23T14:46:33.311911Z","shell.execute_reply":"2023-05-23T14:46:33.644978Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAArYUlEQVR4nO3df3RU9Z3/8deEkASRSQg0M8waQnQtAiIoSIw/sYwESFk4ZavRSFM3QtdNbJGuAqfy2zaALCBsCsXDL7cg6lawoo3EIKTVEDCQFSKmaBGidJJtYzIklBDI/f7h5n4dE5DQiZlPfD7OuQfm83nfez+ffGaYFzd3EodlWZYAAAAMEtbRAwAAAGgrAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDjhHT2A9tLU1KSTJ0+qR48ecjgcHT0cAABwCSzL0qlTp+TxeBQWduHrLJ02wJw8eVLx8fEdPQwAAHAZKioqdNVVV12wv9MGmB49ekj6/AvgdDo7eDQAAOBS+P1+xcfH2+/jF9JpA0zzt42cTicBBgAAw3zV7R/cxAsAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgnPCOHgAAXI5+M1/r6CG02ceLUjt6CECnwRUYAABgHAIMAAAwTpsDTGFhocaPHy+PxyOHw6Ht27dfsPZf//Vf5XA4tGLFioD26upqpaeny+l0KiYmRpmZmaqrqwuoee+993THHXcoKipK8fHxWrJkSVuHCgAAOqk2B5j6+noNGTJEubm5F63btm2b9u7dK4/H06IvPT1dZWVlys/P144dO1RYWKipU6fa/X6/X6NHj1ZCQoJKSkr09NNPa968eVq7dm1bhwsAADqhNt/EO3bsWI0dO/aiNZ9++qkeffRRvfHGG0pNDbxp7ciRI8rLy9P+/fs1fPhwSdKqVas0btw4LV26VB6PR5s3b9bZs2e1fv16RUREaNCgQSotLdWyZcsCgg4AAPhmCvo9ME1NTZo8ebIef/xxDRo0qEV/UVGRYmJi7PAiSV6vV2FhYSouLrZr7rzzTkVERNg1KSkpKi8v12effRbsIQMAAMME/WPUixcvVnh4uH784x+32u/z+RQXFxc4iPBwxcbGyufz2TWJiYkBNS6Xy+7r2bNni+M2NDSooaHBfuz3+/+ueQAAgNAV1CswJSUleuaZZ7Rx40Y5HI5gHvor5eTkKDo62t7i4+O/1vMDAICvT1ADzO9//3tVVVWpb9++Cg8PV3h4uI4fP66f/vSn6tevnyTJ7XarqqoqYL9z586purpabrfbrqmsrAyoaX7cXPNls2bNUm1trb1VVFQEc2oAACCEBPVbSJMnT5bX6w1oS0lJ0eTJk/XQQw9JkpKTk1VTU6OSkhINGzZMkrRr1y41NTUpKSnJrvnZz36mxsZGde3aVZKUn5+v/v37t/rtI0mKjIxUZGRkMKcDAABCVJsDTF1dnT788EP78bFjx1RaWqrY2Fj17dtXvXr1Cqjv2rWr3G63+vfvL0kaMGCAxowZoylTpmjNmjVqbGxUdna20tLS7I9cP/DAA5o/f74yMzM1Y8YMHT58WM8884yWL1/+98wVAAB0Em0OMO+++67uvvtu+/H06dMlSRkZGdq4ceMlHWPz5s3Kzs7WqFGjFBYWpkmTJmnlypV2f3R0tHbu3KmsrCwNGzZMvXv31pw5c/gINQAAkCQ5LMuyOnoQ7cHv9ys6Olq1tbVyOp0dPRwAQcYvcwQ6p0t9/+Z3IQEAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACM0+YAU1hYqPHjx8vj8cjhcGj79u12X2Njo2bMmKHBgwere/fu8ng8+sEPfqCTJ08GHKO6ulrp6elyOp2KiYlRZmam6urqAmree+893XHHHYqKilJ8fLyWLFlyeTMEAACdTpsDTH19vYYMGaLc3NwWfadPn9aBAwc0e/ZsHThwQC+//LLKy8v1T//0TwF16enpKisrU35+vnbs2KHCwkJNnTrV7vf7/Ro9erQSEhJUUlKip59+WvPmzdPatWsvY4oAAKCzcViWZV32zg6Htm3bpokTJ16wZv/+/RoxYoSOHz+uvn376siRIxo4cKD279+v4cOHS5Ly8vI0btw4ffLJJ/J4PFq9erV+9rOfyefzKSIiQpI0c+ZMbd++XR988MEljc3v9ys6Olq1tbVyOp2XO0UAIarfzNc6eght9vGi1I4eAhDyLvX9u93vgamtrZXD4VBMTIwkqaioSDExMXZ4kSSv16uwsDAVFxfbNXfeeacdXiQpJSVF5eXl+uyzz1o9T0NDg/x+f8AGAAA6p3YNMGfOnNGMGTN0//332ynK5/MpLi4uoC48PFyxsbHy+Xx2jcvlCqhpftxc82U5OTmKjo62t/j4+GBPBwAAhIh2CzCNjY269957ZVmWVq9e3V6nsc2aNUu1tbX2VlFR0e7nBAAAHSO8PQ7aHF6OHz+uXbt2BXwPy+12q6qqKqD+3Llzqq6ultvttmsqKysDapofN9d8WWRkpCIjI4M5DQAAEKKCfgWmObwcPXpUb775pnr16hXQn5ycrJqaGpWUlNhtu3btUlNTk5KSkuyawsJCNTY22jX5+fnq37+/evbsGewhAwAAw7Q5wNTV1am0tFSlpaWSpGPHjqm0tFQnTpxQY2Oj/vmf/1nvvvuuNm/erPPnz8vn88nn8+ns2bOSpAEDBmjMmDGaMmWK9u3bp7ffflvZ2dlKS0uTx+ORJD3wwAOKiIhQZmamysrK9MILL+iZZ57R9OnTgzdzAABgrDZ/jHr37t26++67W7RnZGRo3rx5SkxMbHW/t956SyNHjpT0+Q+yy87O1quvvqqwsDBNmjRJK1eu1JVXXmnXv/fee8rKytL+/fvVu3dvPfroo5oxY8Ylj5OPUQOdGx+jBjqnS33//rt+DkwoI8AAnRsBBuicQubnwAAAAAQbAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwTnhHDwAAgG+6fjNf6+ghtNnHi1I79PxcgQEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4bQ4whYWFGj9+vDwejxwOh7Zv3x7Qb1mW5syZoz59+qhbt27yer06evRoQE11dbXS09PldDoVExOjzMxM1dXVBdS89957uuOOOxQVFaX4+HgtWbKk7bMDAACdUpsDTH19vYYMGaLc3NxW+5csWaKVK1dqzZo1Ki4uVvfu3ZWSkqIzZ87YNenp6SorK1N+fr527NihwsJCTZ061e73+/0aPXq0EhISVFJSoqefflrz5s3T2rVrL2OKAACgswlv6w5jx47V2LFjW+2zLEsrVqzQk08+qQkTJkiSnnvuOblcLm3fvl1paWk6cuSI8vLytH//fg0fPlyStGrVKo0bN05Lly6Vx+PR5s2bdfbsWa1fv14REREaNGiQSktLtWzZsoCgAwAAvpmCeg/MsWPH5PP55PV67bbo6GglJSWpqKhIklRUVKSYmBg7vEiS1+tVWFiYiouL7Zo777xTERERdk1KSorKy8v12WeftXruhoYG+f3+gA0AAHRObb4CczE+n0+S5HK5AtpdLpfd5/P5FBcXFziI8HDFxsYG1CQmJrY4RnNfz549W5w7JydH8+fPD85EvkK/ma99LecJpo8XpXb0EAAACJpO8ymkWbNmqba21t4qKio6ekgAAKCdBDXAuN1uSVJlZWVAe2Vlpd3ndrtVVVUV0H/u3DlVV1cH1LR2jC+e48siIyPldDoDNgAA0DkFNcAkJibK7XaroKDAbvP7/SouLlZycrIkKTk5WTU1NSopKbFrdu3apaamJiUlJdk1hYWFamxstGvy8/PVv3//Vr99BAAAvlnaHGDq6upUWlqq0tJSSZ/fuFtaWqoTJ07I4XBo2rRpeuqpp/Tb3/5Whw4d0g9+8AN5PB5NnDhRkjRgwACNGTNGU6ZM0b59+/T2228rOztbaWlp8ng8kqQHHnhAERERyszMVFlZmV544QU988wzmj59etAmDgAAzNXmm3jfffdd3X333fbj5lCRkZGhjRs36oknnlB9fb2mTp2qmpoa3X777crLy1NUVJS9z+bNm5Wdna1Ro0YpLCxMkyZN0sqVK+3+6Oho7dy5U1lZWRo2bJh69+6tOXPm8BFqAAAg6TICzMiRI2VZ1gX7HQ6HFixYoAULFlywJjY2Vlu2bLnoeW644Qb9/ve/b+vwAADAN0Cn+RQSAAD45iDAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADBO0APM+fPnNXv2bCUmJqpbt2665pprtHDhQlmWZddYlqU5c+aoT58+6tatm7xer44ePRpwnOrqaqWnp8vpdComJkaZmZmqq6sL9nABAICBgh5gFi9erNWrV+s///M/deTIES1evFhLlizRqlWr7JolS5Zo5cqVWrNmjYqLi9W9e3elpKTozJkzdk16errKysqUn5+vHTt2qLCwUFOnTg32cAEAgIHCg33Ad955RxMmTFBqaqokqV+/fnr++ee1b98+SZ9ffVmxYoWefPJJTZgwQZL03HPPyeVyafv27UpLS9ORI0eUl5en/fv3a/jw4ZKkVatWady4cVq6dKk8Hk+whw0AAAwS9Cswt956qwoKCvTHP/5RkvQ///M/+sMf/qCxY8dKko4dOyafzyev12vvEx0draSkJBUVFUmSioqKFBMTY4cXSfJ6vQoLC1NxcXGr521oaJDf7w/YAABA5xT0KzAzZ86U3+/Xddddpy5duuj8+fP6+c9/rvT0dEmSz+eTJLlcroD9XC6X3efz+RQXFxc40PBwxcbG2jVflpOTo/nz5wd7OgAAIAQF/QrMiy++qM2bN2vLli06cOCANm3apKVLl2rTpk3BPlWAWbNmqba21t4qKira9XwAAKDjBP0KzOOPP66ZM2cqLS1NkjR48GAdP35cOTk5ysjIkNvtliRVVlaqT58+9n6VlZUaOnSoJMntdquqqirguOfOnVN1dbW9/5dFRkYqMjIy2NMBAAAhKOhXYE6fPq2wsMDDdunSRU1NTZKkxMREud1uFRQU2P1+v1/FxcVKTk6WJCUnJ6umpkYlJSV2za5du9TU1KSkpKRgDxkAABgm6Fdgxo8fr5///Ofq27evBg0apIMHD2rZsmX6l3/5F0mSw+HQtGnT9NRTT+naa69VYmKiZs+eLY/Ho4kTJ0qSBgwYoDFjxmjKlClas2aNGhsblZ2drbS0ND6BBAAAgh9gVq1apdmzZ+vf/u3fVFVVJY/Hox/96EeaM2eOXfPEE0+ovr5eU6dOVU1NjW6//Xbl5eUpKirKrtm8ebOys7M1atQohYWFadKkSVq5cmWwhwsAAAzksL74I3I7Eb/fr+joaNXW1srpdAb12P1mvhbU430dPl6U2tFDAIKK1yE6E57P/9+lvn/zu5AAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjNMuAebTTz/Vgw8+qF69eqlbt24aPHiw3n33XbvfsizNmTNHffr0Ubdu3eT1enX06NGAY1RXVys9PV1Op1MxMTHKzMxUXV1dewwXAAAYJugB5rPPPtNtt92mrl276ne/+53ef/99/cd//Id69uxp1yxZskQrV67UmjVrVFxcrO7duyslJUVnzpyxa9LT01VWVqb8/Hzt2LFDhYWFmjp1arCHCwAADBQe7AMuXrxY8fHx2rBhg92WmJho/92yLK1YsUJPPvmkJkyYIEl67rnn5HK5tH37dqWlpenIkSPKy8vT/v37NXz4cEnSqlWrNG7cOC1dulQejyfYwwYAAAYJ+hWY3/72txo+fLi+//3vKy4uTjfeeKOeffZZu//YsWPy+Xzyer12W3R0tJKSklRUVCRJKioqUkxMjB1eJMnr9SosLEzFxcXBHjIAADBM0APMn/70J61evVrXXnut3njjDT3yyCP68Y9/rE2bNkmSfD6fJMnlcgXs53K57D6fz6e4uLiA/vDwcMXGxto1X9bQ0CC/3x+wAQCAzino30JqamrS8OHD9Ytf/EKSdOONN+rw4cNas2aNMjIygn06W05OjubPn99uxwcAAKEj6Fdg+vTpo4EDBwa0DRgwQCdOnJAkud1uSVJlZWVATWVlpd3ndrtVVVUV0H/u3DlVV1fbNV82a9Ys1dbW2ltFRUVQ5gMAAEJP0APMbbfdpvLy8oC2P/7xj0pISJD0+Q29brdbBQUFdr/f71dxcbGSk5MlScnJyaqpqVFJSYlds2vXLjU1NSkpKanV80ZGRsrpdAZsAACgcwr6t5Aee+wx3XrrrfrFL36he++9V/v27dPatWu1du1aSZLD4dC0adP01FNP6dprr1ViYqJmz54tj8ejiRMnSvr8is2YMWM0ZcoUrVmzRo2NjcrOzlZaWhqfQAIAAMEPMDfffLO2bdumWbNmacGCBUpMTNSKFSuUnp5u1zzxxBOqr6/X1KlTVVNTo9tvv115eXmKioqyazZv3qzs7GyNGjVKYWFhmjRpklauXBns4QIAAAMFPcBI0ne/+11997vfvWC/w+HQggULtGDBggvWxMbGasuWLe0xPAAAYDh+FxIAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOO0eYBYtWiSHw6Fp06bZbWfOnFFWVpZ69eqlK6+8UpMmTVJlZWXAfidOnFBqaqquuOIKxcXF6fHHH9e5c+fae7gAAMAA7Rpg9u/fr1/96le64YYbAtofe+wxvfrqq3rppZe0Z88enTx5Ut/73vfs/vPnzys1NVVnz57VO++8o02bNmnjxo2aM2dOew4XAAAYot0CTF1dndLT0/Xss8+qZ8+ednttba3WrVunZcuW6Tvf+Y6GDRumDRs26J133tHevXslSTt37tT777+vX//61xo6dKjGjh2rhQsXKjc3V2fPnm2vIQMAAEO0W4DJyspSamqqvF5vQHtJSYkaGxsD2q+77jr17dtXRUVFkqSioiINHjxYLpfLrklJSZHf71dZWVmr52toaJDf7w/YAABA5xTeHgfdunWrDhw4oP3797fo8/l8ioiIUExMTEC7y+WSz+eza74YXpr7m/tak5OTo/nz5wdh9AAAINQF/QpMRUWFfvKTn2jz5s2KiooK9uEvaNasWaqtrbW3ioqKr+3cAADg6xX0AFNSUqKqqirddNNNCg8PV3h4uPbs2aOVK1cqPDxcLpdLZ8+eVU1NTcB+lZWVcrvdkiS3293iU0nNj5trviwyMlJOpzNgAwAAnVPQA8yoUaN06NAhlZaW2tvw4cOVnp5u/71r164qKCiw9ykvL9eJEyeUnJwsSUpOTtahQ4dUVVVl1+Tn58vpdGrgwIHBHjIAADBM0O+B6dGjh66//vqAtu7du6tXr152e2ZmpqZPn67Y2Fg5nU49+uijSk5O1i233CJJGj16tAYOHKjJkydryZIl8vl8evLJJ5WVlaXIyMhgDxkAABimXW7i/SrLly9XWFiYJk2apIaGBqWkpOiXv/yl3d+lSxft2LFDjzzyiJKTk9W9e3dlZGRowYIFHTFcAAAQYr6WALN79+6Ax1FRUcrNzVVubu4F90lISNDrr7/eziMDAAAm4nchAQAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABgn6AEmJydHN998s3r06KG4uDhNnDhR5eXlATVnzpxRVlaWevXqpSuvvFKTJk1SZWVlQM2JEyeUmpqqK664QnFxcXr88cd17ty5YA8XAAAYKOgBZs+ePcrKytLevXuVn5+vxsZGjR49WvX19XbNY489pldffVUvvfSS9uzZo5MnT+p73/ue3X/+/Hmlpqbq7Nmzeuedd7Rp0yZt3LhRc+bMCfZwAQCAgcKDfcC8vLyAxxs3blRcXJxKSkp05513qra2VuvWrdOWLVv0ne98R5K0YcMGDRgwQHv37tUtt9yinTt36v3339ebb74pl8uloUOHauHChZoxY4bmzZuniIiIYA8bAAAYpN3vgamtrZUkxcbGSpJKSkrU2Ngor9dr11x33XXq27evioqKJElFRUUaPHiwXC6XXZOSkiK/36+ysrJWz9PQ0CC/3x+wAQCAzqldA0xTU5OmTZum2267Tddff70kyefzKSIiQjExMQG1LpdLPp/PrvlieGnub+5rTU5OjqKjo+0tPj4+yLMBAAChol0DTFZWlg4fPqytW7e252kkSbNmzVJtba29VVRUtPs5AQBAxwj6PTDNsrOztWPHDhUWFuqqq66y291ut86ePauampqAqzCVlZVyu912zb59+wKO1/wppeaaL4uMjFRkZGSQZwEAAEJR0K/AWJal7Oxsbdu2Tbt27VJiYmJA/7Bhw9S1a1cVFBTYbeXl5Tpx4oSSk5MlScnJyTp06JCqqqrsmvz8fDmdTg0cODDYQwYAAIYJ+hWYrKwsbdmyRa+88op69Ohh37MSHR2tbt26KTo6WpmZmZo+fbpiY2PldDr16KOPKjk5WbfccoskafTo0Ro4cKAmT56sJUuWyOfz6cknn1RWVhZXWQAAQPADzOrVqyVJI0eODGjfsGGDfvjDH0qSli9frrCwME2aNEkNDQ1KSUnRL3/5S7u2S5cu2rFjhx555BElJyere/fuysjI0IIFC4I9XAAAYKCgBxjLsr6yJioqSrm5ucrNzb1gTUJCgl5//fVgDg0AAHQS/C4kAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOOEdIDJzc1Vv379FBUVpaSkJO3bt6+jhwQAAEJAyAaYF154QdOnT9fcuXN14MABDRkyRCkpKaqqqurooQEAgA4WsgFm2bJlmjJlih566CENHDhQa9as0RVXXKH169d39NAAAEAHC+/oAbTm7NmzKikp0axZs+y2sLAweb1eFRUVtbpPQ0ODGhoa7Me1tbWSJL/fH/TxNTWcDvox21t7fB2AjsTrEJ0Jz+eWx7Us66J1IRlg/vKXv+j8+fNyuVwB7S6XSx988EGr++Tk5Gj+/Pkt2uPj49tljKaJXtHRIwDA6xCdSXs/n0+dOqXo6OgL9odkgLkcs2bN0vTp0+3HTU1Nqq6uVq9eveRwOIJ2Hr/fr/j4eFVUVMjpdAbtuKGks8+R+Zmvs8+xs89P6vxzZH6Xz7IsnTp1Sh6P56J1IRlgevfurS5duqiysjKgvbKyUm63u9V9IiMjFRkZGdAWExPTXkOU0+nslE/KL+rsc2R+5uvsc+zs85M6/xyZ3+W52JWXZiF5E29ERISGDRumgoICu62pqUkFBQVKTk7uwJEBAIBQEJJXYCRp+vTpysjI0PDhwzVixAitWLFC9fX1euihhzp6aAAAoIOFbIC577779L//+7+aM2eOfD6fhg4dqry8vBY39n7dIiMjNXfu3BbfrupMOvscmZ/5OvscO/v8pM4/R+bX/hzWV31OCQAAIMSE5D0wAAAAF0OAAQAAxiHAAAAA4xBgAACAcQgwknJzc9WvXz9FRUUpKSlJ+/btu2j9Sy+9pOuuu05RUVEaPHiwXn/99YB+y7I0Z84c9enTR926dZPX69XRo0fbcwoX1Zb5Pfvss7rjjjvUs2dP9ezZU16vt0X9D3/4QzkcjoBtzJgx7T2Ni2rLHDdu3Nhi/FFRUQE1Jq/hyJEjW8zP4XAoNTXVrgmlNSwsLNT48ePl8XjkcDi0ffv2r9xn9+7duummmxQZGal//Md/1MaNG1vUtPV13V7aOr+XX35Z99xzj771rW/J6XQqOTlZb7zxRkDNvHnzWqzfdddd146zuLi2znH37t2tPkd9Pl9Analr2Nrry+FwaNCgQXZNKK1hTk6Obr75ZvXo0UNxcXGaOHGiysvLv3K/jn4v/MYHmBdeeEHTp0/X3LlzdeDAAQ0ZMkQpKSmqqqpqtf6dd97R/fffr8zMTB08eFATJ07UxIkTdfjwYbtmyZIlWrlypdasWaPi4mJ1795dKSkpOnPmzNc1LVtb57d7927df//9euutt1RUVKT4+HiNHj1an376aUDdmDFj9Oc//9nenn/++a9jOq1q6xylz3965BfHf/z48YB+k9fw5ZdfDpjb4cOH1aVLF33/+98PqAuVNayvr9eQIUOUm5t7SfXHjh1Tamqq7r77bpWWlmratGl6+OGHA97kL+c50V7aOr/CwkLdc889ev3111VSUqK7775b48eP18GDBwPqBg0aFLB+f/jDH9pj+JekrXNsVl5eHjCHuLg4u8/kNXzmmWcC5lVRUaHY2NgWr8FQWcM9e/YoKytLe/fuVX5+vhobGzV69GjV19dfcJ+QeC+0vuFGjBhhZWVl2Y/Pnz9veTweKycnp9X6e++910pNTQ1oS0pKsn70ox9ZlmVZTU1Nltvttp5++mm7v6amxoqMjLSef/75dpjBxbV1fl927tw5q0ePHtamTZvstoyMDGvChAnBHupla+scN2zYYEVHR1/weJ1tDZcvX2716NHDqqurs9tCbQ2bSbK2bdt20ZonnnjCGjRoUEDbfffdZ6WkpNiP/96vWXu5lPm1ZuDAgdb8+fPtx3PnzrWGDBkSvIEF0aXM8a233rIkWZ999tkFazrTGm7bts1yOBzWxx9/bLeF8hpWVVVZkqw9e/ZcsCYU3gu/0Vdgzp49q5KSEnm9XrstLCxMXq9XRUVFre5TVFQUUC9JKSkpdv2xY8fk8/kCaqKjo5WUlHTBY7aXy5nfl50+fVqNjY2KjY0NaN+9e7fi4uLUv39/PfLII/rrX/8a1LFfqsudY11dnRISEhQfH68JEyaorKzM7utsa7hu3TqlpaWpe/fuAe2hsoZt9VWvwWB8zUJJU1OTTp061eI1ePToUXk8Hl199dVKT0/XiRMnOmiEl2/o0KHq06eP7rnnHr399tt2e2dbw3Xr1snr9SohISGgPVTXsLa2VpJaPOe+KBTeC7/RAeYvf/mLzp8/3+Kn+7pcrhbfi23m8/kuWt/8Z1uO2V4uZ35fNmPGDHk8noAn4ZgxY/Tcc8+poKBAixcv1p49ezR27FidP38+qOO/FJczx/79+2v9+vV65ZVX9Otf/1pNTU269dZb9cknn0jqXGu4b98+HT58WA8//HBAeyitYVtd6DXo9/v1t7/9LSjP+1CydOlS1dXV6d5777XbkpKStHHjRuXl5Wn16tU6duyY7rjjDp06daoDR3rp+vTpozVr1ug3v/mNfvOb3yg+Pl4jR47UgQMHJAXn365QcfLkSf3ud79r8RoM1TVsamrStGnTdNttt+n666+/YF0ovBeG7K8SQMdbtGiRtm7dqt27dwfc5JqWlmb/ffDgwbrhhht0zTXXaPfu3Ro1alRHDLVNkpOTA34p6K233qoBAwboV7/6lRYuXNiBIwu+devWafDgwRoxYkRAu+lr+E2xZcsWzZ8/X6+88krA/SFjx461/37DDTcoKSlJCQkJevHFF5WZmdkRQ22T/v37q3///vbjW2+9VR999JGWL1+u//qv/+rAkQXfpk2bFBMTo4kTJwa0h+oaZmVl6fDhwx16T9Wl+kZfgendu7e6dOmiysrKgPbKykq53e5W93G73Retb/6zLcdsL5czv2ZLly7VokWLtHPnTt1www0Xrb366qvVu3dvffjhh3/3mNvq75ljs65du+rGG2+0x99Z1rC+vl5bt269pH8MO3IN2+pCr0Gn06lu3boF5TkRCrZu3aqHH35YL774YotL9V8WExOjb3/720as34WMGDHCHn9nWUPLsrR+/XpNnjxZERERF60NhTXMzs7Wjh079NZbb+mqq666aG0ovBd+owNMRESEhg0bpoKCArutqalJBQUFAf9D/6Lk5OSAeknKz8+36xMTE+V2uwNq/H6/iouLL3jM9nI585M+v3N84cKFysvL0/Dhw7/yPJ988on++te/qk+fPkEZd1tc7hy/6Pz58zp06JA9/s6whtLnH3FsaGjQgw8++JXn6cg1bKuveg0G4znR0Z5//nk99NBDev755wM+/n4hdXV1+uijj4xYvwspLS21x98Z1lD6/NM9H3744SX9J6Ij19CyLGVnZ2vbtm3atWuXEhMTv3KfkHgvDMqtwAbbunWrFRkZaW3cuNF6//33ralTp1oxMTGWz+ezLMuyJk+ebM2cOdOuf/vtt63w8HBr6dKl1pEjR6y5c+daXbt2tQ4dOmTXLFq0yIqJibFeeeUV67333rMmTJhgJSYmWn/7299Cfn6LFi2yIiIirP/+7/+2/vznP9vbqVOnLMuyrFOnTln//u//bhUVFVnHjh2z3nzzTeumm26yrr32WuvMmTNf+/wuZ47z58+33njjDeujjz6ySkpKrLS0NCsqKsoqKyuza0xew2a33367dd9997VoD7U1PHXqlHXw4EHr4MGDliRr2bJl1sGDB63jx49blmVZM2fOtCZPnmzX/+lPf7KuuOIK6/HHH7eOHDli5ebmWl26dLHy8vLsmq/6moXy/DZv3myFh4dbubm5Aa/Bmpoau+anP/2ptXv3buvYsWPW22+/bXm9Xqt3795WVVXV1z4/y2r7HJcvX25t377dOnr0qHXo0CHrJz/5iRUWFma9+eabdo3Ja9jswQcftJKSklo9Ziit4SOPPGJFR0dbu3fvDnjOnT592q4JxffCb3yAsSzLWrVqldW3b18rIiLCGjFihLV3716776677rIyMjIC6l988UXr29/+thUREWENGjTIeu211wL6m5qarNmzZ1sul8uKjIy0Ro0aZZWXl38dU2lVW+aXkJBgSWqxzZ0717Isyzp9+rQ1evRo61vf+pbVtWtXKyEhwZoyZUqH/KPyRW2Z47Rp0+xal8tljRs3zjpw4EDA8UxeQ8uyrA8++MCSZO3cubPFsUJtDZs/UvvlrXlOGRkZ1l133dVin6FDh1oRERHW1VdfbW3YsKHFcS/2Nfs6tXV+d91110XrLevzj4336dPHioiIsP7hH/7Buu+++6wPP/zw653YF7R1josXL7auueYaKyoqyoqNjbVGjhxp7dq1q8VxTV1Dy/r8I8PdunWz1q5d2+oxQ2kNW5ubpIDXVSi+Fzr+b/AAAADG+EbfAwMAAMxEgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcf4flV2Zk+l1sPMAAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:03.833311Z","iopub.execute_input":"2023-05-23T11:59:03.834255Z","iopub.status.idle":"2023-05-23T11:59:15.408455Z","shell.execute_reply.started":"2023-05-23T11:59:03.834217Z","shell.execute_reply":"2023-05-23T11:59:15.407224Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Using pretrained model on English form HuggingFace","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                          do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:15.418724Z","iopub.execute_input":"2023-05-23T11:59:15.420163Z","iopub.status.idle":"2023-05-23T11:59:16.495673Z","shell.execute_reply.started":"2023-05-23T11:59:15.420134Z","shell.execute_reply":"2023-05-23T11:59:16.494777Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae697abf2ac4717b1c1c6e2e721b45f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8194f5bc55f44139091753df1ea33ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae6baa7d2a1485d90fb929da671d7a5"}},"metadata":{}}]},{"cell_type":"markdown","source":"Preprocessing data\n\nFull description in notebook FinalWork_v1","metadata":{}},{"cell_type":"code","source":"def text_cleaning(text):\n    text = str(text).lower()\n    text = ' '.join([elem for elem in text.split(' ') if elem.isalpha()])\n    text = re.sub(re.compile(r'https?://\\S+|www\\.\\S+'), '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    return text\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in (stopwords.words('english') + ['ha', 'wa']))\n    return text\n\ndef update_text(text):\n    new_words = []\n    flag = False\n    text = contractions.fix(text)\n    words = text.split()\n    lem = WordNetLemmatizer()\n    for word in words:\n        if flag:\n            word = 'not_' + word\n            flag = False\n        if word in ['not', 'no', 'never', \"n't\"]:\n            flag = True\n        else:\n            new_words.append(lem.lemmatize(word))\n    return ' '.join(new_words)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:16.521723Z","iopub.execute_input":"2023-05-23T11:59:16.522284Z","iopub.status.idle":"2023-05-23T11:59:16.532627Z","shell.execute_reply.started":"2023-05-23T11:59:16.522247Z","shell.execute_reply":"2023-05-23T11:59:16.531741Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pip install git+https://github.com/huggingface/accelerate","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:16.533956Z","iopub.execute_input":"2023-05-23T11:59:16.534720Z","iopub.status.idle":"2023-05-23T11:59:43.594981Z","shell.execute_reply.started":"2023-05-23T11:59:16.534686Z","shell.execute_reply":"2023-05-23T11:59:43.593722Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/accelerate\n  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-tzg_58_h\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-tzg_58_h\n  Resolved https://github.com/huggingface/accelerate to commit 9a86a49f72f5c9c0ca521b903fb1a44fa8ee8f53\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (5.9.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (1.23.5)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.0.dev0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.20.0.dev0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.11.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (4.5.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (1.11.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.0.dev0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.0.dev0) (1.3.0)\nBuilding wheels for collected packages: accelerate\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.20.0.dev0-py3-none-any.whl size=225635 sha256=d77280e05f0f84788ce92b92f5ff69d69cbd5740083f163c2c803ccde3e83aaa\n  Stored in directory: /tmp/pip-ephem-wheel-cache-nj1vp97b/wheels/f6/c7/9d/1b8a5ca8353d9307733bc719107acb67acdc95063bba749f26\nSuccessfully built accelerate\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.20.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:43.597042Z","iopub.execute_input":"2023-05-23T11:59:43.597410Z","iopub.status.idle":"2023-05-23T11:59:45.014401Z","shell.execute_reply.started":"2023-05-23T11:59:43.597370Z","shell.execute_reply":"2023-05-23T11:59:45.013066Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"cleaning data for train and test df","metadata":{}},{"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda x: text_cleaning(x))\ntrain_df['text'] = train_df['text'].apply(lambda x: update_text(x))\ntrain_df['text'] = train_df['text'].apply(lambda x: remove_stopwords(x))   ","metadata":{"execution":{"iopub.status.busy":"2023-05-23T11:59:45.025438Z","iopub.execute_input":"2023-05-23T11:59:45.026088Z","iopub.status.idle":"2023-05-23T12:00:26.137359Z","shell.execute_reply.started":"2023-05-23T11:59:45.026054Z","shell.execute_reply":"2023-05-23T12:00:26.136395Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"test_df['text'] = test_df['text'].apply(lambda x: text_cleaning(x))\ntest_df['text'] = test_df['text'].apply(lambda x: update_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x: remove_stopwords(x))  ","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:26.139083Z","iopub.execute_input":"2023-05-23T12:00:26.139452Z","iopub.status.idle":"2023-05-23T12:00:30.961929Z","shell.execute_reply.started":"2023-05-23T12:00:26.139417Z","shell.execute_reply":"2023-05-23T12:00:30.960910Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Using a tokenizer to encode a batch of texts for training and validation (and add some special tokens) \n\n* add_special_tokens = True - adding tokens such as CLS and SEP\n\n* return_attention_mask = True - returns attention masks (indicates which tokens need to be attented and which ones need to be ignored)\n\n* pad_to_max_length = True - all sequences will have the same length (max_length)\n\n* return_tensors = 'pt'- format of returned tensors (PyTorch tensors)","metadata":{}},{"cell_type":"code","source":"train_texts = list(train_df.text)\ntest_texts = list(test_df.text)\n\nencoded_data_train = tokenizer.batch_encode_plus(\n    train_texts, add_special_tokens=True, return_attention_mask=True, \n    pad_to_max_length=True, max_length=512, return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    test_texts, add_special_tokens=True, return_attention_mask=True, \n    pad_to_max_length=True, max_length=512, return_tensors='pt'\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:30.963272Z","iopub.execute_input":"2023-05-23T12:00:30.963640Z","iopub.status.idle":"2023-05-23T12:00:56.372777Z","shell.execute_reply.started":"2023-05-23T12:00:30.963604Z","shell.execute_reply":"2023-05-23T12:00:56.371807Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(train_df.sentiment.values.tolist())\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(test_df.sentiment.values.tolist())","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:56.374332Z","iopub.execute_input":"2023-05-23T12:00:56.374682Z","iopub.status.idle":"2023-05-23T12:00:56.391426Z","shell.execute_reply.started":"2023-05-23T12:00:56.374647Z","shell.execute_reply":"2023-05-23T12:00:56.390406Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"building datasets for working with tensors","metadata":{}},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:56.392839Z","iopub.execute_input":"2023-05-23T12:00:56.393340Z","iopub.status.idle":"2023-05-23T12:00:56.409280Z","shell.execute_reply.started":"2023-05-23T12:00:56.393304Z","shell.execute_reply":"2023-05-23T12:00:56.408469Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"creating custom dataset instead of previous one (alternative version)","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels, unique_labels):\n        self.encodings = encodings\n        self.labels = labels\n        self.unique_labels = unique_labels\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n        labels = self.unique_labels * [0]\n        labels[self.labels[idx]] = 1\n        item[\"label\"] = torch.tensor(labels).float()\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:56.410753Z","iopub.execute_input":"2023-05-23T12:00:56.411199Z","iopub.status.idle":"2023-05-23T12:00:56.421762Z","shell.execute_reply.started":"2023-05-23T12:00:56.411163Z","shell.execute_reply":"2023-05-23T12:00:56.420907Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(encoded_data_train, labels_train, 3)\nval_dataset = Dataset(encoded_data_val, labels_val, 3)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:56.423314Z","iopub.execute_input":"2023-05-23T12:00:56.423714Z","iopub.status.idle":"2023-05-23T12:00:56.434746Z","shell.execute_reply.started":"2023-05-23T12:00:56.423650Z","shell.execute_reply":"2023-05-23T12:00:56.433862Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"* num_train_epochs - number of training epochs\n\n* per_device_train_batch_size - setting batch size for training, number of training examples which will processed parallel\n\n* per_device_eval_batch_size - setting batch size for evaluation, number of evaluation examples which will processed parallel\n\n* weight_decay - regularization technique to prevent overfitting\n\n* load_best_model_at_end - best model according to the metrcis will be saved at the end of the training\n\n* learning_rate - learning rate, which controls step size during training\n\n* evaluation_strategy - model will be evaluated at the end of epoch\n\n* save_strategy - model will be saved at the end of the epoch\n\n* seed - To ensure reproducibility we can set the random seed of this parameter. Using seeds for random processes during model training guarantees determinism and reproducibility","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results_pred', num_train_epochs=3,\n    per_device_train_batch_size=8, per_device_eval_batch_size=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True, \n    learning_rate=5e-5,\n    evaluation_strategy='epoch', save_strategy='epoch',\n    save_total_limit=1,\n    seed=21\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:56.436199Z","iopub.execute_input":"2023-05-23T12:00:56.436730Z","iopub.status.idle":"2023-05-23T12:00:56.533384Z","shell.execute_reply.started":"2023-05-23T12:00:56.436691Z","shell.execute_reply":"2023-05-23T12:00:56.532373Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"using well writen function of metrics from source","metadata":{}},{"cell_type":"code","source":"# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\ndef multi_label_metrics(predictions, labels, threshold=0.5):\n    sigmoid = torch.nn.Sigmoid()\n    probs = sigmoid(torch.Tensor(predictions))\n    y_pred = np.zeros(probs.shape)\n    y_pred[np.where(probs >= threshold)] = 1\n    y_true = labels\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n    accuracy = accuracy_score(y_true, y_pred)\n    metrics = {'f1': f1_micro_average,\n               'roc_auc': roc_auc,\n               'accuracy': accuracy}\n    return metrics\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds, \n        labels=p.label_ids)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:00:56.534823Z","iopub.execute_input":"2023-05-23T12:00:56.535200Z","iopub.status.idle":"2023-05-23T12:00:56.543582Z","shell.execute_reply.started":"2023-05-23T12:00:56.535165Z","shell.execute_reply":"2023-05-23T12:00:56.542423Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=3,\n                                                      problem_type=\"multi_label_classification\",\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:08:32.803491Z","iopub.execute_input":"2023-05-23T14:08:32.803910Z","iopub.status.idle":"2023-05-23T14:08:36.907098Z","shell.execute_reply.started":"2023-05-23T14:08:32.803877Z","shell.execute_reply":"2023-05-23T14:08:36.906056Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model, tokenizer=tokenizer, train_dataset=train_dataset,\n    eval_dataset=val_dataset, args=training_args,\n    compute_metrics=compute_metrics\n)          ","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:01:00.843243Z","iopub.execute_input":"2023-05-23T12:01:00.843599Z","iopub.status.idle":"2023-05-23T12:01:08.541153Z","shell.execute_reply.started":"2023-05-23T12:01:00.843563Z","shell.execute_reply":"2023-05-23T12:01:08.540175Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"****************d1a1196c3912141e779fb72c320e9c31a5dc28a3","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T12:01:08.542777Z","iopub.execute_input":"2023-05-23T12:01:08.543198Z","iopub.status.idle":"2023-05-23T13:17:39.383819Z","shell.execute_reply.started":"2023-05-23T12:01:08.543161Z","shell.execute_reply":"2023-05-23T13:17:39.382574Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230523_120121-ghxvpt8s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/finals/huggingface/runs/ghxvpt8s' target=\"_blank\">deep-durian-15</a></strong> to <a href='https://wandb.ai/finals/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/finals/huggingface' target=\"_blank\">https://wandb.ai/finals/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/finals/huggingface/runs/ghxvpt8s' target=\"_blank\">https://wandb.ai/finals/huggingface/runs/ghxvpt8s</a>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/3976564599.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5154' max='5154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5154/5154 1:15:37, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Roc Auc</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.443300</td>\n      <td>0.417762</td>\n      <td>0.702695</td>\n      <td>0.776104</td>\n      <td>0.678834</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.377600</td>\n      <td>0.425357</td>\n      <td>0.710221</td>\n      <td>0.781834</td>\n      <td>0.691285</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.279600</td>\n      <td>0.493336</td>\n      <td>0.699841</td>\n      <td>0.774406</td>\n      <td>0.685625</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/3976564599.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/tmp/ipykernel_31/3976564599.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5154, training_loss=0.37599469156379983, metrics={'train_runtime': 4590.4787, 'train_samples_per_second': 17.96, 'train_steps_per_second': 1.123, 'total_flos': 2.1691859497749504e+16, 'train_loss': 0.37599469156379983, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"In order to get accuracy per class (positive, neutral, negative) it is needed to find prediction","metadata":{}},{"cell_type":"code","source":"def get_prediction_from_trainer():\n    test_prediction = trainer.predict(val_dataset)\n    labels = np.argmax(test_prediction.predictions, axis = -1)\n    return labels","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:50:25.302700Z","iopub.execute_input":"2023-05-23T14:50:25.303072Z","iopub.status.idle":"2023-05-23T14:50:25.310338Z","shell.execute_reply.started":"2023-05-23T14:50:25.303040Z","shell.execute_reply":"2023-05-23T14:50:25.308334Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"pred = get_prediction_from_trainer()\nprint(pred)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:50:27.390412Z","iopub.execute_input":"2023-05-23T14:50:27.390783Z","iopub.status.idle":"2023-05-23T14:51:42.228638Z","shell.execute_reply.started":"2023-05-23T14:50:27.390735Z","shell.execute_reply":"2023-05-23T14:51:42.227276Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3976564599.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"[1 2 0 ... 1 2 2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"function accuracy_per_class computes and prints the accuracy per class and overall accuracy based on the provided predictions and true values","metadata":{}},{"cell_type":"code","source":"def accuracy_per_class(predictions, true_vals):\n    label_dict = {0: 'Class 0', 1: 'Class 1', 2: 'Class 2'}\n    class_counts = {label: {'correct': 0, 'total': 0} for label in label_dict.values()}\n\n    for pred, true in zip(predictions, true_vals):\n        pred_label, true_label = label_dict[pred], label_dict[true]\n        class_counts[pred_label]['total'] += 1\n        if pred_label == true_label:\n            class_counts[pred_label]['correct'] += 1\n\n    for label, counts in class_counts.items():\n        accuracy = counts['correct'] / counts['total']\n        print(f\"Class: {label}\")\n        print(f\"Accuracy: {counts['correct']} / {counts['total']} = {accuracy:.4f}\\n\")\n\n    overall_accuracy = accuracy_score(true_vals, predictions)\n    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_per_class(pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T14:54:26.298269Z","iopub.execute_input":"2023-05-23T14:54:26.298623Z","iopub.status.idle":"2023-05-23T14:54:26.320474Z","shell.execute_reply.started":"2023-05-23T14:54:26.298594Z","shell.execute_reply":"2023-05-23T14:54:26.319553Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Class: Class 0\nAccuracy: 668 / 919 = 0.7269\n\nClass: Class 1\nAccuracy: 1070 / 1660 = 0.6446\n\nClass: Class 2\nAccuracy: 772 / 955 = 0.8084\n\nOverall Accuracy: 0.7102\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can see that the accuracy of the model exceeds the previous algorithm in notebook FinalWork_v1. We obtained: 71% accuracy.","metadata":{}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T13:17:39.385244Z","iopub.execute_input":"2023-05-23T13:17:39.385890Z","iopub.status.idle":"2023-05-23T13:18:53.501586Z","shell.execute_reply.started":"2023-05-23T13:17:39.385850Z","shell.execute_reply":"2023-05-23T13:18:53.500169Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3976564599.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.41776183247566223,\n 'eval_f1': 0.7026947861745753,\n 'eval_roc_auc': 0.7761035653650256,\n 'eval_accuracy': 0.678834182229768,\n 'eval_runtime': 74.1021,\n 'eval_samples_per_second': 47.691,\n 'eval_steps_per_second': 4.777,\n 'epoch': 3.0}"},"metadata":{}}]}]}